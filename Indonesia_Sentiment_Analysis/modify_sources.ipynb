{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_key= ''\n",
    "access_secret = ''\n",
    "\n",
    "start_date = '2020-03-20'\n",
    "end_date = '2020-03-20'\n",
    "\n",
    "#pass twitter credentials to tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "a = tweepy.Cursor(api.search, q='PSBB',\n",
    "              count=1, include_rts=False, since=start_date, tweet_mode=\"extended\").pages(1)\n",
    "for page in a:\n",
    "    for status in page:\n",
    "        public_tweets = status\n",
    "        \n",
    "import json \n",
    "\n",
    "status = public_tweets\n",
    "\n",
    "#convert to string\n",
    "json_str = json.dumps(status._json)\n",
    "\n",
    "#deserialise string into python object\n",
    "parsed = json.loads(json_str)\n",
    "\n",
    "b = json.dumps(parsed, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "file_2  = open(\"cleaning_source/combined_slang_words.txt\", \"r\")\n",
    "content2 = file_2.read()\n",
    "slang_words = ast.literal_eval(content2)\n",
    "file_2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1017"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(slang_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'wtb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m slang_words[\u001b[39m'\u001b[39;49m\u001b[39mwtb\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'wtb'"
     ]
    }
   ],
   "source": [
    "slang_words['wtb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_words['gak']  = 'tidak'\n",
    "slang_words['trus'] = 'lalu'\n",
    "slang_words['dlm'] = 'dalam'\n",
    "slang_words['tetep'] = 'tetap'\n",
    "slang_words['skrg'] = 'sekarang'\n",
    "slang_words['sm'] = 'dengan'\n",
    "slang_words['udh'] = 'sudah'\n",
    "slang_words['cm'] = 'cuma'\n",
    "slang_words['org'] = 'orang'\n",
    "slang_words['bangor'] = 'nakal'\n",
    "slang_words['ngamuk'] = 'mengamuk'\n",
    "slang_words['iso'] = 'bisa'\n",
    "slang_words['mbuh'] = 'tidak tahu'\n",
    "slang_words['kzl'] = 'kesal'\n",
    "slang_words['name'] = 'nama'\n",
    "slang_words['sdh'] = 'sudah'\n",
    "slang_words['bts'] = 'base transceiver station'\n",
    "slang_words['hut'] = 'hari ulang tahun'\n",
    "slang_words['min'] = 'admin'\n",
    "slang_words['stelh'] = 'setelah'\n",
    "slang_words['utma'] = 'utama'\n",
    "slang_words['hbis'] = 'habis'\n",
    "slang_words['klihatan'] = 'kelihatan'\n",
    "slang_words['mmg'] = 'memang'\n",
    "slang_words['sbenarx'] = 'sebenarnya'\n",
    "slang_words['sedkit'] = 'sedikit'\n",
    "slang_words['dipake'] = 'dipakai'\n",
    "slang_words['ilang2an'] = 'hilang hilang'\n",
    "slang_words['dm'] = 'direct message'\n",
    "slang_words['bjir'] = 'anjing'\n",
    "slang_words['pls'] = 'please'\n",
    "slang_words['anehny'] = 'anehnya'\n",
    "slang_words['wtb'] = 'want to buy'\n",
    "slang_words['wts'] = 'want to sell'\n",
    "slang_words['ngajak'] = 'mengajak'\n",
    "slang_words['kenapaaaaaaaaa'] = 'kenapa'\n",
    "slang_words['gmn'] = 'gimana'\n",
    "slang_words['dn'] ='dan'\n",
    "slang_words['pdhl'] = 'padahal'\n",
    "slang_words['dmn'] = 'dimana'\n",
    "slang_words['penngen'] = 'pengen'\n",
    "slang_words['smph'] = 'sampah'\n",
    "slang_words['w'] = 'saya'\n",
    "slang_words['lahi'] = 'lagi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"cleaning_source/update_combined_slang_words.txt\",\"w\")\n",
    "f.write( str(slang_words) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "file_2  = open(\"cleaning_source/update_combined_slang_words.txt\", \"r\")\n",
    "content2 = file_2.read()\n",
    "update_slang_words = ast.literal_eval(content2)\n",
    "file_2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1052"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(update_slang_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "import pytz \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lexicon = pd.read_csv('lexicon/InSet-master/positive.tsv',sep='\\t')\n",
    "neg_lexicon = pd.read_csv('lexicon/InSet-master/negative.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hai</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>merekam</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ekstensif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paripurna</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>detail</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>melarikan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605</th>\n",
       "      <td>vakansi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>spesial</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3607</th>\n",
       "      <td>asrama</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>orisinal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3609 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  weight\n",
       "0           hai       3\n",
       "1       merekam       2\n",
       "2     ekstensif       3\n",
       "3     paripurna       1\n",
       "4        detail       2\n",
       "...         ...     ...\n",
       "3604  melarikan       3\n",
       "3605    vakansi       3\n",
       "3606    spesial       4\n",
       "3607     asrama       3\n",
       "3608   orisinal       3\n",
       "\n",
       "[3609 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>putus tali gantung</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gelebah</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gobar hati</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tersentuh (perasaan)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>isak</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6604</th>\n",
       "      <td>kantong kering</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6605</th>\n",
       "      <td>penggaruk</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6606</th>\n",
       "      <td>senewen</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6607</th>\n",
       "      <td>menetapkan</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6608</th>\n",
       "      <td>kacang botor</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6609 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      word  weight\n",
       "0       putus tali gantung      -2\n",
       "1                  gelebah      -2\n",
       "2               gobar hati      -2\n",
       "3     tersentuh (perasaan)      -1\n",
       "4                     isak      -5\n",
       "...                    ...     ...\n",
       "6604        kantong kering      -4\n",
       "6605             penggaruk      -3\n",
       "6606               senewen      -4\n",
       "6607            menetapkan      -5\n",
       "6608          kacang botor      -3\n",
       "\n",
       "[6609 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t2/lw7m_8k914jgqsvv48dm5gsm0000gn/T/ipykernel_47831/2205863292.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_lexicon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_lexicon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "lexicon = pos_lexicon.append(neg_lexicon,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = pd.concat([pos_lexicon, neg_lexicon], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10218"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "addition = pd.read_csv('lexicon/agusmakmun/sentimentword.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word      0\n",
       "weight    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word      0\n",
       "weight    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_word = lexicon['word'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon['word'][0] in lexicon_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_word = []\n",
    "add_weight = []\n",
    "for i in range(0,len(addition)):\n",
    "    if (addition['word'][i] not in lexicon_word):\n",
    "        add_word.append(addition['word'][i])\n",
    "        add_weight.append(addition['weight'][i])\n",
    "\n",
    "addition_lexicon = pd.DataFrame(list(zip(add_word,add_weight)),columns =['word','weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acuh</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agresi</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ajaib</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>akal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alarm</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>waktunya</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>waspadalah</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>wow</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>wtf</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>yatim</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>826 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  weight\n",
       "0          acuh      -1\n",
       "1        agresi      -3\n",
       "2         ajaib       3\n",
       "3          akal       2\n",
       "4         alarm      -3\n",
       "..          ...     ...\n",
       "821    waktunya      -2\n",
       "822  waspadalah      -2\n",
       "823         wow       3\n",
       "824         wtf      -3\n",
       "825       yatim      -2\n",
       "\n",
       "[826 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon = pd.concat([lexicon, addition_lexicon], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t2/lw7m_8k914jgqsvv48dm5gsm0000gn/T/ipykernel_47831/3197821956.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_lexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddition_lexicon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# full_lexicon = lexicon.append(addition_lexicon,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9672</th>\n",
       "      <td>gurub</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4422</th>\n",
       "      <td>babi</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>gapaian</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>kemayu</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7831</th>\n",
       "      <td>kerugian</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  weight\n",
       "9672     gurub      -5\n",
       "4422      babi      -4\n",
       "448    gapaian       3\n",
       "4667    kemayu      -3\n",
       "7831  kerugian      -5"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_lexicon.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word      zuhud\n",
       "weight       -1\n",
       "dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_lexicon[full_lexicon['weight']<0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open(\"lexicon/swearwords/swear-words.txt\", \"r\")\n",
    "content = my_file.read()\n",
    "swear_words = content.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_swear = [-5 for i in range(len(swear_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "swear_lexicon = pd.DataFrame(list(zip(swear_words,weight_swear)),columns =['word','weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anjing</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anjiang</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anjir</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anjay</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asu</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>sontoloyo</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>tai</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>telek</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>tolol</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td></td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  weight\n",
       "0      anjing      -5\n",
       "1     anjiang      -5\n",
       "2       anjir      -5\n",
       "3       anjay      -5\n",
       "4         asu      -5\n",
       "..        ...     ...\n",
       "64  sontoloyo      -5\n",
       "65        tai      -5\n",
       "66      telek      -5\n",
       "67      tolol      -5\n",
       "68                 -5\n",
       "\n",
       "[69 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swear_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon = lexicon.append(swear_lexicon,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon = pd.concat([full_lexicon, swear_lexicon], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11112"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you want to remove the row at index 11112 from the DataFrame full_lexicon\n",
    "full_lexicon = full_lexicon.drop(11112)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_words(x):\n",
    "    words = word_tokenize(x['word'])\n",
    "    number = len(words)\n",
    "    return number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m full_lexicon[\u001b[39m'\u001b[39m\u001b[39mnumber_of_words\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m full_lexicon[\u001b[39m'\u001b[39;49m\u001b[39mword\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: number_of_words(x),axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/pandas/core/series.py:4631\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4523\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4526\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4527\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4528\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4530\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4629\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4630\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4631\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/pandas/core/apply.py:133\u001b[0m, in \u001b[0;36mApply.__init__.<locals>.f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(x):\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mreturn\u001b[39;00m func(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "full_lexicon['number_of_words'] = full_lexicon['word'].apply(lambda x: number_of_words(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/afa/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon['word'] = full_lexicon['word'].astype(str).fillna('')\n",
    "full_lexicon['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_lexicon['word'] = full_lexicon['word'].astype(str)\n",
    "\n",
    "def number_of_words(x):\n",
    "    words = word_tokenize(x)\n",
    "    number = len(words)\n",
    "    return number \n",
    "\n",
    "full_lexicon['number_of_words'] = full_lexicon['word'].apply(number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m full_lexicon \u001b[39m=\u001b[39m full_lexicon\u001b[39m.\u001b[39mdrop(full_lexicon[full_lexicon[\u001b[39m'\u001b[39;49m\u001b[39mnumber_of_words\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mindex[\u001b[39m0\u001b[39;49m],axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/pandas/core/indexes/base.py:5174\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5171\u001b[0m \u001b[39mif\u001b[39;00m is_integer(key) \u001b[39mor\u001b[39;00m is_float(key):\n\u001b[1;32m   5172\u001b[0m     \u001b[39m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[1;32m   5173\u001b[0m     key \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mcast_scalar_indexer(key)\n\u001b[0;32m-> 5174\u001b[0m     \u001b[39mreturn\u001b[39;00m getitem(key)\n\u001b[1;32m   5176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n\u001b[1;32m   5177\u001b[0m     \u001b[39m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[1;32m   5178\u001b[0m     \u001b[39m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[1;32m   5179\u001b[0m     result \u001b[39m=\u001b[39m getitem(key)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "full_lexicon = full_lexicon.drop(full_lexicon[full_lexicon['number_of_words'] == 0].index[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon  = full_lexicon.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "      <th>number_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [word, weight, number_of_words]\n",
       "Index: []"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_lexicon[full_lexicon['number_of_words'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon.to_csv(r\"lexicon/full_lexicon_afa.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "      <th>number_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hai</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>merekam</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ekstensif</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paripurna</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>detail</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11107</th>\n",
       "      <td>silit</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11108</th>\n",
       "      <td>sontoloyo</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11109</th>\n",
       "      <td>tai</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11110</th>\n",
       "      <td>telek</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111</th>\n",
       "      <td>tolol</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11112 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  weight  number_of_words\n",
       "0            hai       3                1\n",
       "1        merekam       2                1\n",
       "2      ekstensif       3                1\n",
       "3      paripurna       1                1\n",
       "4         detail       2                1\n",
       "...          ...     ...              ...\n",
       "11107      silit      -5                1\n",
       "11108  sontoloyo      -5                1\n",
       "11109        tai      -5                1\n",
       "11110      telek      -5                1\n",
       "11111      tolol      -5                1\n",
       "\n",
       "[11112 rows x 3 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_lexicon = pd.read_csv('lexicon/modified_full_lexicon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m modified_lexicon[\u001b[39m'\u001b[39m\u001b[39mnumber_of_words\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m modified_lexicon\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: number_of_words(x),axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/pandas/core/frame.py:9428\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9417\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9419\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9420\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9421\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9426\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9427\u001b[0m )\n\u001b[0;32m-> 9428\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/pandas/core/apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    676\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/pandas/core/apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    800\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/pandas/core/apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    812\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    813\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    815\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    816\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    817\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[116], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m modified_lexicon[\u001b[39m'\u001b[39m\u001b[39mnumber_of_words\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m modified_lexicon\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: number_of_words(x),axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[106], line 4\u001b[0m, in \u001b[0;36mnumber_of_words\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumber_of_words\u001b[39m(x):\n\u001b[0;32m----> 4\u001b[0m     words \u001b[39m=\u001b[39m word_tokenize(x)\n\u001b[1;32m      5\u001b[0m     number \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(words)\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m number\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/nltk/tokenize/__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1278\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1328\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1329\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[1;32m   1330\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1459\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1460\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[1;32m   1461\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/nltk/tokenize/punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[1;32m    320\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    322\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mslice\u001b[39m]:\n\u001b[1;32m   1430\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1431\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[1;32m   1432\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[1;32m   1433\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n",
      "File \u001b[0;32m~/anaconda3/envs/Sentiment/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m previous_slice \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m   1394\u001b[0m previous_match \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1395\u001b[0m \u001b[39mfor\u001b[39;00m match \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang_vars\u001b[39m.\u001b[39;49mperiod_context_re()\u001b[39m.\u001b[39;49mfinditer(text):\n\u001b[1;32m   1396\u001b[0m \n\u001b[1;32m   1397\u001b[0m     \u001b[39m# Get the slice of the previous word\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     before_text \u001b[39m=\u001b[39m text[previous_slice\u001b[39m.\u001b[39mstop : match\u001b[39m.\u001b[39mstart()]\n\u001b[1;32m   1399\u001b[0m     index_after_last_space \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "modified_lexicon['number_of_words'] = modified_lexicon.apply(lambda x: number_of_words(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_lexicon.to_csv(r'lexicon\\modified_full_lexicon.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open(\"cleaning_source/combined_stop_words.txt\", \"r\")\n",
    "content = my_file.read()\n",
    "stop_words = content.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_word = modified_lexicon['word'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "      <th>number_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hai</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>merekam</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ekstensif</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paripurna</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>detail</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10245</th>\n",
       "      <td>sontoloyo</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10246</th>\n",
       "      <td>tai</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10247</th>\n",
       "      <td>telek</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10248</th>\n",
       "      <td>tolol</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10249</th>\n",
       "      <td>budi baik</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10250 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  weight  number_of_words\n",
       "0            hai       3                1\n",
       "1        merekam       2                1\n",
       "2      ekstensif       3                1\n",
       "3      paripurna       1                1\n",
       "4         detail       2                1\n",
       "...          ...     ...              ...\n",
       "10245  sontoloyo      -5                1\n",
       "10246        tai      -5                1\n",
       "10247      telek      -5                1\n",
       "10248      tolol      -5                1\n",
       "10249  budi baik       3                2\n",
       "\n",
       "[10250 rows x 3 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap=[]\n",
    "for word in stop_words:\n",
    "    if word in lexicon_word:\n",
    "        overlap.append(word)\n",
    "    else:\n",
    "        kata_dasar = stemmer.stem(word)\n",
    "        if kata_dasar in lexicon_word:\n",
    "            overlap.append(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akulah',\n",
       " 'amatlah',\n",
       " 'andalah',\n",
       " 'apa',\n",
       " 'artinya',\n",
       " 'awalnya',\n",
       " 'bagi',\n",
       " 'balik',\n",
       " 'begini',\n",
       " 'beginikah',\n",
       " 'bekerja',\n",
       " 'benarkah',\n",
       " 'berada',\n",
       " 'berarti',\n",
       " 'berbagai',\n",
       " 'beri',\n",
       " 'berikut',\n",
       " 'berkata',\n",
       " 'berkeinginan',\n",
       " 'berlainan',\n",
       " 'berlangsung',\n",
       " 'bermaksud',\n",
       " 'bersama',\n",
       " 'bersiap',\n",
       " 'bertanya',\n",
       " 'berturut',\n",
       " 'bertutur',\n",
       " 'berupa',\n",
       " 'betul',\n",
       " 'biasa',\n",
       " 'boleh',\n",
       " 'bolehlah',\n",
       " 'bulan',\n",
       " 'cukup',\n",
       " 'cukuplah',\n",
       " 'dahulu',\n",
       " 'dari',\n",
       " 'datang',\n",
       " 'dia',\n",
       " 'diberi',\n",
       " 'diberikannya',\n",
       " 'dibuatnya',\n",
       " 'didatangkan',\n",
       " 'diingat',\n",
       " 'dijelaskan',\n",
       " 'dikarenakan',\n",
       " 'dikatakannya',\n",
       " 'diketahui',\n",
       " 'dikira',\n",
       " 'dilalui',\n",
       " 'dimaksud',\n",
       " 'dimaksudkannya',\n",
       " 'diminta',\n",
       " 'dimulailah',\n",
       " 'dimungkinkan',\n",
       " 'dipastikan',\n",
       " 'diperbuatnya',\n",
       " 'diperkirakan',\n",
       " 'diperlukan',\n",
       " 'dipersoalkan',\n",
       " 'dipunyai',\n",
       " 'dirinya',\n",
       " 'disinilah',\n",
       " 'ditanyai',\n",
       " 'dituturkannya',\n",
       " 'diucapkannya',\n",
       " 'entah',\n",
       " 'guna',\n",
       " 'hanya',\n",
       " 'haruslah',\n",
       " 'hendak',\n",
       " 'hendaknya',\n",
       " 'ikut',\n",
       " 'ingat-ingat',\n",
       " 'itukah',\n",
       " 'jadi',\n",
       " 'jadinya',\n",
       " 'jauh',\n",
       " 'jawaban',\n",
       " 'jelas',\n",
       " 'jelaslah',\n",
       " 'kalaulah',\n",
       " 'karenanya',\n",
       " 'kata',\n",
       " 'katakanlah',\n",
       " 'kebetulan',\n",
       " 'kedua',\n",
       " 'kelihatan',\n",
       " 'kemungkinan',\n",
       " 'kinilah',\n",
       " 'kira-kira',\n",
       " 'lagi',\n",
       " 'lainnya',\n",
       " 'lama',\n",
       " 'lanjut',\n",
       " 'lebih',\n",
       " 'mampu',\n",
       " 'masalah',\n",
       " 'mau',\n",
       " 'melainkan',\n",
       " 'melalui',\n",
       " 'melihatnya',\n",
       " 'memastikan',\n",
       " 'memberikan',\n",
       " 'memerlukan',\n",
       " 'meminta',\n",
       " 'mempergunakan',\n",
       " 'memperlihatkan',\n",
       " 'mempunyai',\n",
       " 'memungkinkan',\n",
       " 'menambahkan',\n",
       " 'menanti',\n",
       " 'menantikan',\n",
       " 'menanyai',\n",
       " 'mendapat',\n",
       " 'mendatang',\n",
       " 'mendatangkan',\n",
       " 'mengatakan',\n",
       " 'mengenai',\n",
       " 'mengetahui',\n",
       " 'menghendaki',\n",
       " 'mengingatkan',\n",
       " 'mengira',\n",
       " 'mengucapkannya',\n",
       " 'menjadi',\n",
       " 'menjelaskan',\n",
       " 'menunjukkan',\n",
       " 'menurut',\n",
       " 'menyampaikan',\n",
       " 'menyatakan',\n",
       " 'merasa',\n",
       " 'meski',\n",
       " 'meyakini',\n",
       " 'minta',\n",
       " 'mulai',\n",
       " 'mulanya',\n",
       " 'mungkinkah',\n",
       " 'naik',\n",
       " 'nanti',\n",
       " 'pada',\n",
       " 'padanya',\n",
       " 'pasti',\n",
       " 'penting',\n",
       " 'perlunya',\n",
       " 'pertanyakan',\n",
       " 'pihaknya',\n",
       " 'punya',\n",
       " 'rasanya',\n",
       " 'rupanya',\n",
       " 'sajalah',\n",
       " 'sama',\n",
       " 'sampai-sampai',\n",
       " 'saya',\n",
       " 'sebabnya',\n",
       " 'sebagian',\n",
       " 'sebaik-baiknya',\n",
       " 'sebaliknya',\n",
       " 'sebegini',\n",
       " 'sebenarnya',\n",
       " 'sebesar',\n",
       " 'secukupnya',\n",
       " 'sedikit',\n",
       " 'seharusnya',\n",
       " 'seingat',\n",
       " 'sejauh',\n",
       " 'sekadarnya',\n",
       " 'sekali-kali',\n",
       " 'sekecil',\n",
       " 'sekurangnya',\n",
       " 'selama-lamanya',\n",
       " 'selanjutnya',\n",
       " 'seluruhnya',\n",
       " 'semakin',\n",
       " 'semampunya',\n",
       " 'semata-mata',\n",
       " 'sementara',\n",
       " 'semua',\n",
       " 'semula',\n",
       " 'sendirian',\n",
       " 'sepantasnya',\n",
       " 'seperlunya',\n",
       " 'sepertinya',\n",
       " 'sering',\n",
       " 'serta',\n",
       " 'sesudah',\n",
       " 'setengah',\n",
       " 'setidaknya',\n",
       " 'seusai',\n",
       " 'siap',\n",
       " 'siapakah',\n",
       " 'soal',\n",
       " 'sudahkah',\n",
       " 'supaya',\n",
       " 'tambah',\n",
       " 'tampak',\n",
       " 'tanyakan',\n",
       " 'tempat',\n",
       " 'tentang',\n",
       " 'tentulah',\n",
       " 'terasa',\n",
       " 'terdahulu',\n",
       " 'terdiri',\n",
       " 'teringat-ingat',\n",
       " 'terjadilah',\n",
       " 'terkira',\n",
       " 'terlebih',\n",
       " 'termasuk',\n",
       " 'tersampaikan',\n",
       " 'tertuju',\n",
       " 'terutama',\n",
       " 'tinggi',\n",
       " 'tutur',\n",
       " 'ucap',\n",
       " 'ujar',\n",
       " 'umum',\n",
       " 'ungkap',\n",
       " 'usai',\n",
       " 'wah',\n",
       " 'walau',\n",
       " 'yakin',\n",
       " 'yang',\n",
       " 'ada',\n",
       " 'adanya',\n",
       " 'anda',\n",
       " 'atas',\n",
       " 'baik',\n",
       " 'beginilah',\n",
       " 'buat',\n",
       " 'cuma',\n",
       " 'dapat',\n",
       " 'dialah',\n",
       " 'harus',\n",
       " 'hanyalah',\n",
       " 'harusnya',\n",
       " 'hendaklah',\n",
       " 'itu',\n",
       " 'itulah',\n",
       " 'jangan',\n",
       " 'kalau',\n",
       " 'karena',\n",
       " 'kiranya',\n",
       " 'langsung',\n",
       " 'lamanya',\n",
       " 'makin',\n",
       " 'mungkinlah',\n",
       " 'nantinya',\n",
       " 'paparnya',\n",
       " 'pun',\n",
       " 'pastilah',\n",
       " 'saja',\n",
       " 'sampai',\n",
       " 'sebab',\n",
       " 'sendiri',\n",
       " 'seperti',\n",
       " 'sebanyak',\n",
       " 'sebetulnya',\n",
       " 'sedikitnya',\n",
       " 'sekali',\n",
       " 'selagi',\n",
       " 'semuanya',\n",
       " 'sendirinya',\n",
       " 'sepanjang',\n",
       " 'sepantasnyalah',\n",
       " 'serupa',\n",
       " 'sesama',\n",
       " 'sesudahnya',\n",
       " 'tetap',\n",
       " 'tentu',\n",
       " 'tentunya',\n",
       " 'tertentu',\n",
       " 'tidakkah',\n",
       " 'waduh',\n",
       " 'nyatanya',\n",
       " 'para',\n",
       " 'pentingnya',\n",
       " 'percuma',\n",
       " 'perlukah',\n",
       " 'pertanyaan',\n",
       " 'pihak',\n",
       " 'pukul',\n",
       " 'rasa',\n",
       " 'rata',\n",
       " 'sama-sama',\n",
       " 'sampaikan',\n",
       " 'satu',\n",
       " 'sayalah',\n",
       " 'sebagai',\n",
       " 'sebaik',\n",
       " 'sebaiknya',\n",
       " 'sebuah',\n",
       " 'segera',\n",
       " 'sejenak',\n",
       " 'sekadar',\n",
       " 'sekalian',\n",
       " 'sekurang-kurangnya',\n",
       " 'selama',\n",
       " 'selamanya',\n",
       " 'semampu',\n",
       " 'seringnya',\n",
       " 'sesegera',\n",
       " 'setempat',\n",
       " 'setidak-tidaknya',\n",
       " 'siapa',\n",
       " 'siapapun',\n",
       " 'soalnya',\n",
       " 'sudah',\n",
       " 'sudahlah',\n",
       " 'tahu',\n",
       " 'tambahnya',\n",
       " 'tampaknya',\n",
       " 'tanya',\n",
       " 'tanyanya',\n",
       " 'tengah',\n",
       " 'terbanyak',\n",
       " 'terdapat',\n",
       " 'teringat',\n",
       " 'terjadi',\n",
       " 'terjadinya',\n",
       " 'terlalu',\n",
       " 'terlihat',\n",
       " 'ternyata',\n",
       " 'turut',\n",
       " 'tuturnya',\n",
       " 'ucapnya',\n",
       " 'ujarnya',\n",
       " 'umumnya',\n",
       " 'ungkapnya',\n",
       " 'usah',\n",
       " 'dua',\n",
       " 'lalu',\n",
       " 'lain',\n",
       " 'banyak',\n",
       " 'besar',\n",
       " 'merupakan',\n",
       " 'agar',\n",
       " 'persen',\n",
       " 'diri',\n",
       " 'hubungan',\n",
       " 'hidup',\n",
       " 'mantan',\n",
       " 'tinggal',\n",
       " 'sesuai',\n",
       " 'memberi',\n",
       " 'mencari',\n",
       " 'ruang',\n",
       " 'biasanya',\n",
       " 'berdasarkan',\n",
       " 'membawa',\n",
       " 'tingkat',\n",
       " 'dekat',\n",
       " 'membantu',\n",
       " 'ditemukan',\n",
       " 'kegiatan',\n",
       " 'tampil',\n",
       " 'bertemu',\n",
       " 'milik',\n",
       " 'menjalani',\n",
       " 'upaya',\n",
       " 'mengambil',\n",
       " 'lewat',\n",
       " 'meningkatkan',\n",
       " 'kehidupan',\n",
       " 'penggunaan',\n",
       " 'aku',\n",
       " 'kurang',\n",
       " 'ya',\n",
       " 'amat',\n",
       " 'seraya',\n",
       " 'tolong']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
